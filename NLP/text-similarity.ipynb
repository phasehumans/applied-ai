{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21093,
     "status": "ok",
     "timestamp": 1763086731275,
     "user": {
      "displayName": "Chaitanya",
      "userId": "17149693889078725148"
     },
     "user_tz": -330
    },
    "id": "cZjjFHr7ViOL",
    "outputId": "7fc6e456-11d9-4a37-81f4-5a6409b9941a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def text_similarity(text1, text2):\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(text1)\n",
    "    tokens2 = word_tokenize(text2)\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens1 = [lemmatizer.lemmatize(token.lower()) for token in tokens1 if token.isalpha()]\n",
    "    tokens2 = [lemmatizer.lemmatize(token.lower()) for token in tokens2 if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
    "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
    "\n",
    "    # Join tokens back into strings\n",
    "    text1_processed = \" \".join(tokens1)\n",
    "    text2_processed = \" \".join(tokens2)\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1_processed, text2_processed])\n",
    "\n",
    "    # âœ… Correct way to compute cosine similarity\n",
    "    similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "text1 = \"Natural Language Processing makes computers understand human language.\"\n",
    "text2 = \"NLP enables machines to comprehend what people say or write.\"\n",
    "print(\"Similarity Score:\", text_similarity(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWf387ibKBh3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
